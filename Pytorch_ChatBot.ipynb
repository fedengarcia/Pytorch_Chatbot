{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "339457cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\feden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Importamos dataset y dataloader, para procesar los datos para ingresarlos a los modelos\n",
    "# Nos permite usar nuestros datos para generar nuestro mdoelo de entrenamiento, agrupa los ejemplos con sus respectivas etiquetas.\n",
    "# Dataloader, convierte dataset a un objeto iterable para pytorch, nos permite realizar nuestros propios minibatches de forma automatica \n",
    "# De forma simple con dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2699211",
   "metadata": {},
   "source": [
    "### DEFINE NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0a4aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.l4 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l4(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e1c15b",
   "metadata": {},
   "source": [
    "### Methods\n",
    "    For clean data\n",
    "    Tokenize sentence\n",
    "    Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00876a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#REMOVE ACCENTS\n",
    "def removeAccents(word):\n",
    "    translation = str.maketrans(\"áéíóúüàèìùò\", \"aeiouuaeiou\")\n",
    "    return word.translate(translation)\n",
    "\n",
    "\n",
    "#REMOVE PUNCTUATION AND SINGS\n",
    "def removePunctuation (word):\n",
    "    word = word.translate(str.maketrans('', '', string.punctuation))   \n",
    "    return word\n",
    "\n",
    "\n",
    "#TOKENIZE WORDS\n",
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "# BAG OF WORDS\n",
    "def bag_of_words(tokenized_sentence, words):\n",
    "    \"\"\"\n",
    "    return bag of words array:\n",
    "    1 for each known word that exists in the sentence, 0 otherwise\n",
    "    example:\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize bag with 0 for each word\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in tokenized_sentence: \n",
    "            bag[idx] = 1\n",
    "\n",
    "    return bag\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0d1531",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a43bc6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 patterns: [(['hola'], 'CC_HOLA'), (['hi'], 'CC_HOLA'), (['hello'], 'CC_HOLA'), (['ciao'], 'CC_HOLA'), (['buen', 'dia'], 'CC_HOLA'), (['buenas', 'tardes'], 'CC_HOLA'), (['buenas', 'noches'], 'CC_HOLA'), (['hey'], 'CC_HOLA'), (['whats', 'up'], 'CC_HOLA'), (['que', 'onda'], 'CC_HOLA'), (['hola', 'que', 'tal'], 'CC_HOLA'), (['hola', 'amigo'], 'CC_HOLA'), (['hola', 'holaa'], 'CC_HOLA'), (['cya'], 'CC_ADIOS'), (['nos', 'vemos'], 'CC_ADIOS'), (['Goodbye'], 'CC_ADIOS'), (['chau'], 'CC_ADIOS'), (['adios'], 'CC_ADIOS'), (['hasta', 'luego'], 'CC_ADIOS'), (['me', 'voy'], 'CC_ADIOS'), (['Cuantos', 'anios'], 'CC_EDAD'), (['Cuantos', 'aÃ±os', 'tenes'], 'CC_EDAD'), (['cual', 'es', 'tu', 'edad'], 'CC_EDAD'), (['que', 'tan', 'viejo', 'eres'], 'CC_EDAD'), (['edad', '?'], 'CC_EDAD'), (['tu', 'edad'], 'CC_EDAD'), (['dime', 'tus', 'aÃ±os'], 'CC_EDAD'), (['como', 'te', 'apodaron'], 'CC_NAME_BOT'), (['como', 'te', 'llamas'], 'CC_NAME_BOT'), (['cual', 'es', 'tu', 'nombre'], 'CC_NAME_BOT'), (['como', 'te', 'dicen'], 'CC_NAME_BOT'), (['tenes', 'un', 'nombre'], 'CC_NAME_BOT'), (['decime', 'tu', 'nombre'], 'CC_NAME_BOT'), (['tu', 'nombre', 'como', 'es'], 'CC_NAME_BOT'), (['quien', 'sos'], 'CC_NAME_BOT'), (['que', 'haces'], 'CC_PARA_QUE_FUE_CREADO'), (['para', 'que', 'te', 'crearon'], 'CC_PARA_QUE_FUE_CREADO'), (['cual', 'es', 'tu', 'funcion'], 'CC_PARA_QUE_FUE_CREADO'), (['a', 'que', 'te', 'dedicas'], 'CC_PARA_QUE_FUE_CREADO'), (['con', 'que', 'me', 'puedes', 'ayudar'], 'CC_PARA_QUE_FUE_CREADO'), (['para', 'que', 'sirves'], 'CC_PARA_QUE_FUE_CREADO'), (['que', 'haces', 'para', 'vivir'], 'CC_PARA_QUE_FUE_CREADO'), (['cual', 'es', 'tu', 'objetivo'], 'CC_PARA_QUE_FUE_CREADO'), (['como', 'estas'], 'CC_COMO_ESTAS'), (['como', 'te', 'sientes', 'hoy'], 'CC_COMO_ESTAS'), (['cual', 'es', 'tu', 'humor'], 'CC_COMO_ESTAS'), (['estas', 'contento'], 'CC_COMO_ESTAS'), (['sos', 'feliz'], 'CC_COMO_ESTAS'), (['como', 'andas'], 'CC_COMO_ESTAS'), (['estas', 'chevere'], 'CC_COMO_ESTAS'), (['clima', 'de', 'hoy'], 'CC_EL_CLIMA'), (['hace', 'frio'], 'CC_EL_CLIMA'), (['hace', 'calor'], 'CC_EL_CLIMA'), (['que', 'temperatura', 'hay', 'aqui'], 'CC_EL_CLIMA'), (['quiero', 'saber', 'si', 'hace', 'frio', 'o', 'calor'], 'CC_EL_CLIMA'), (['como', 'esta', 'el', 'dia'], 'CC_EL_CLIMA')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('chitchat_intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "\n",
    "for intent in intents['intents']:\n",
    "        tag = intent['tag']\n",
    "        \n",
    "        # add to tag list\n",
    "        tags.append(tag)\n",
    "        for pattern in intent['patterns']:\n",
    "            \n",
    "            #Clean data\n",
    "            # tokenize each word in the sentence\n",
    "            words = tokenize(pattern)\n",
    "            \n",
    "            # Remove accents\n",
    "            w = [removeAccents(word) for word in words]\n",
    "\n",
    "            # Remove punctuation\n",
    "            w = [removePunctuation(word) for word in words]\n",
    "\n",
    "            # Remove empty slots    \n",
    "            w = [word for word in words if word != '']\n",
    "            \n",
    "            # add to our words list\n",
    "            all_words.extend(words)\n",
    "            \n",
    "            # add to xy pair\n",
    "            xy.append((words, tag))\n",
    "\n",
    "# Remove duplicates and sort\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "\n",
    "print(f\"{len(xy)} patterns: {xy}\\n\")\n",
    "# print(len(tags), \"tags:\", tags)\n",
    "# print(\"\")\n",
    "# print(len(all_words), \" words:\", all_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50e6df8",
   "metadata": {},
   "source": [
    "### Prepare Data for model\n",
    "    80% of the dataset will be for train the model\n",
    "    20% of the dataset will be for testing the model\n",
    "    We can use random_split from Pytorch or Split the data manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70de6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------CREATING TESTING DATA--------------------\n",
      "\n",
      "\n",
      "---------------CREATING TRAINING DATA--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number_rows = len(xy)    # The size of our dataset or the number of rows in excel table.  \n",
    "# test_split = int(number_rows*0.3)  \n",
    "# validate_split = int(number_rows*0.2) \n",
    "# train_split = number_rows - test_split - validate_split     \n",
    "# train_set, validate_set, test_set = random_split(xy, [train_split, validate_split, test_split])   \n",
    "\n",
    "\n",
    "# Create testing data\n",
    "print(\"\\n---------------CREATING TESTING DATA--------------------\\n\")\n",
    "\n",
    "# Data for testing the model\n",
    "test_array = []\n",
    "percentage = int(len(xy)*0.2)\n",
    "dataset_copy = xy\n",
    "\n",
    "for i in range(percentage):\n",
    "    item = dataset_copy.pop(random.randrange(len(xy)))\n",
    "    test_array.append(item)\n",
    "#     item = xy.pop(random.randrange(len(xy)))\n",
    "#     bag = bag_of_words(item[0], all_words)\n",
    "#     x_test.append(bag)\n",
    "#     label = tags.index(item[1])\n",
    "#     y_test.append(label)\n",
    "\n",
    "\n",
    "# Create training data\n",
    "print(\"\\n---------------CREATING TRAINING DATA--------------------\\n\")\n",
    "\n",
    "# Data for training\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for (pattern_sentence, tag) in dataset_copy:\n",
    "    # X: bag of words for each pattern_sentence\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    x_train.append(bag)\n",
    "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# x_test = np.array(x_test)\n",
    "# y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c073c80c",
   "metadata": {},
   "source": [
    "### TRAINING THE MODEL\n",
    "    Define Hyper Parameters\n",
    "        Epochs,\n",
    "        Batch_size,\n",
    "        Learning_rate,\n",
    "        input, hidden, output sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e430551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- TRAINING DATA --------------------\n",
      "\n",
      "Epoch [100/1000], Loss: 0.2520\n",
      "Epoch [200/1000], Loss: 0.0080\n",
      "Epoch [300/1000], Loss: 0.0022\n",
      "Epoch [400/1000], Loss: 0.0014\n",
      "Epoch [500/1000], Loss: 0.0012\n",
      "Epoch [600/1000], Loss: 0.0003\n",
      "Epoch [700/1000], Loss: 0.0002\n",
      "Epoch [800/1000], Loss: 0.0001\n",
      "Epoch [900/1000], Loss: 0.0001\n",
      "Epoch [1000/1000], Loss: 0.0000\n",
      "\n",
      "Final loss: 0.0000\n",
      "\n",
      "training complete. file saved to data.pth\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters \n",
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "input_size = len(x_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "\n",
    "# Train Class Dataset\n",
    "class ChatTrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(x_train)\n",
    "        self.x_data = x_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "# Define Train Loader\n",
    "train_set = ChatTrainDataset()\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Select device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create neural network\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n--------------- TRAINING DATA --------------------\\n\")\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "            words = words.to(device)\n",
    "            labels = labels.to(dtype=torch.long).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(words)\n",
    "\n",
    "            # if y would be one-hot, we must apply\n",
    "            # labels = torch.max(labels, 1)[1]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "print(f'\\nFinal loss: {loss.item():.4f}\\n')\n",
    "\n",
    "trained_data = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"input_size\": input_size,\n",
    "        \"hidden_size\": hidden_size,\n",
    "        \"output_size\": output_size,\n",
    "        \"all_words\": all_words,\n",
    "        \"tags\": tags\n",
    "}\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "torch.save(trained_data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e4ef9",
   "metadata": {},
   "source": [
    "### TEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ba9591d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EJEMPLO: ['cual', 'es', 'tu', 'funcion'] - Intencion:CC_PARA_QUE_FUE_CREADO\n",
      "PREDICCION del Ejemplo: ['cual', 'es', 'tu', 'funcion'] = Intencion:CC_PARA_QUE_FUE_CREADO con Confianza:0.9999262094497681 \n",
      "\n",
      "EJEMPLO: ['hola', 'amigo'] - Intencion:CC_HOLA\n",
      "PREDICCION del Ejemplo: ['hola', 'amigo'] = Intencion:CC_HOLA con Confianza:0.9999123811721802 \n",
      "\n",
      "EJEMPLO: ['estas', 'chevere'] - Intencion:CC_COMO_ESTAS\n",
      "PREDICCION del Ejemplo: ['estas', 'chevere'] = Intencion:CC_COMO_ESTAS con Confianza:0.9999265670776367 \n",
      "\n",
      "EJEMPLO: ['buenas', 'noches'] - Intencion:CC_HOLA\n",
      "PREDICCION del Ejemplo: ['buenas', 'noches'] = Intencion:CC_HOLA con Confianza:0.9999912977218628 \n",
      "\n",
      "EJEMPLO: ['con', 'que', 'me', 'puedes', 'ayudar'] - Intencion:CC_PARA_QUE_FUE_CREADO\n",
      "PREDICCION del Ejemplo: ['con', 'que', 'me', 'puedes', 'ayudar'] = Intencion:CC_EL_CLIMA con Confianza:0.9794712662696838 \n",
      "\n",
      "EJEMPLO: ['que', 'temperatura', 'hay', 'aqui'] - Intencion:CC_EL_CLIMA\n",
      "PREDICCION del Ejemplo: ['que', 'temperatura', 'hay', 'aqui'] = Intencion:CC_EL_CLIMA con Confianza:0.9999737739562988 \n",
      "\n",
      "EJEMPLO: ['hey'] - Intencion:CC_HOLA\n",
      "PREDICCION del Ejemplo: ['hey'] = Intencion:CC_HOLA con Confianza:0.9999421834945679 \n",
      "\n",
      "EJEMPLO: ['quiero', 'saber', 'si', 'hace', 'frio', 'o', 'calor'] - Intencion:CC_EL_CLIMA\n",
      "PREDICCION del Ejemplo: ['quiero', 'saber', 'si', 'hace', 'frio', 'o', 'calor'] = Intencion:CC_EL_CLIMA con Confianza:0.9999973773956299 \n",
      "\n",
      "EJEMPLO: ['hi'] - Intencion:CC_HOLA\n",
      "PREDICCION del Ejemplo: ['hi'] = Intencion:CC_HOLA con Confianza:0.9999634027481079 \n",
      "\n",
      "EJEMPLO: ['hace', 'frio'] - Intencion:CC_EL_CLIMA\n",
      "PREDICCION del Ejemplo: ['hace', 'frio'] = Intencion:CC_EL_CLIMA con Confianza:0.9937325119972229 \n",
      "\n",
      "EJEMPLO: ['tu', 'nombre', 'como', 'es'] - Intencion:CC_NAME_BOT\n",
      "PREDICCION del Ejemplo: ['tu', 'nombre', 'como', 'es'] = Intencion:CC_NAME_BOT con Confianza:0.9999905824661255 \n",
      "\n",
      "Accuracy de tu modelo: 0.9975297776135531\n",
      "Clasifico correctamente el 100% de los ejemplos para testing\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correcta = 0\n",
    "with torch.no_grad():\n",
    "    for item in test_array:\n",
    "\n",
    "        #Clean data\n",
    "        # tokenize each word in the sentence\n",
    "        pattern = item[0]\n",
    "        \n",
    "        # Remove accents\n",
    "        pattern = [removeAccents(word) for word in pattern]\n",
    "\n",
    "        # Remove punctuation\n",
    "        pattern = [removePunctuation(word) for word in pattern]\n",
    "\n",
    "        # Remove empty slots    \n",
    "        pattern = [word for word in pattern if word != '']\n",
    "            \n",
    "\n",
    "        X = bag_of_words(pattern, all_words)\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "        \n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "        \n",
    "        print(f\"EJEMPLO: {item[0]} - Intencion:{item[1]}\")\n",
    "        print(f\"PREDICCION del Ejemplo: {item[0]} = Intencion:{tag} con Confianza:{prob.item()} \\n\")\n",
    "        total += prob.item()\n",
    "        if(tag == item[1]):\n",
    "            correcta += 1\n",
    "            \n",
    "print(\"Accuracy de tu modelo:\",total / len(test_array))      \n",
    "print(f\"Clasifico correctamente el {int(correcta/0.1)}% de los ejemplos para testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e730342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5460e866",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32b7dceb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# loop thru each data item\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#     get item predictor input values\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     get item target values (0 or 1 or 2 . . )\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#   end-loop\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#   return num correct / (num correct + num wrong)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(train_accu,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-o\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(eval_accu,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-o\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# loop thru each data item\n",
    "#     get item predictor input values\n",
    "#     get item target values (0 or 1 or 2 . . )\n",
    "#     use inputs to compute output logit values \n",
    "    \n",
    "#     find index of largest output logit value\n",
    "#     if index == target\n",
    "#       correct prediction\n",
    "#     else\n",
    "#       wrong prediction\n",
    "#   end-loop\n",
    "#   return num correct / (num correct + num wrong)\n",
    "\n",
    "plt.plot(train_accu,'-o')\n",
    "plt.plot(eval_accu,'-o')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['Train','Valid'])\n",
    "plt.title('Train vs Valid Accuracy')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4489b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e78266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eefbea3d",
   "metadata": {},
   "source": [
    "### CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd0a3b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola! hablemos un rato... (type 'quit' to exit)\n",
      "Tu: hola\n",
      "Confianza:0.9999517202377319 -> Intencion:CC_HOLA\n",
      "WAMAC: Hola! Como puedo ayudarte?\n",
      "Tu: nose, todo bien ?\n",
      "Confianza:0.7963751554489136 -> Intencion:CC_HOLA\n",
      "WAMAC: Hola! Como puedo ayudarte?\n",
      "Tu: como estas\n",
      "Confianza:0.999970555305481 -> Intencion:CC_COMO_ESTAS\n",
      "WAMAC: Me siento muy bien!\n",
      "Tu: como te llamas\n",
      "Confianza:0.9133269786834717 -> Intencion:CC_NAME_BOT\n",
      "WAMAC: No se si ya me pusieron mi nombre, pero escuche rumores de que me llamo WAMAC.\n",
      "Tu: cual es tu nombre\n",
      "Confianza:0.9999804496765137 -> Intencion:CC_NAME_BOT\n",
      "WAMAC: No se si ya me pusieron mi nombre, pero escuche rumores de que me llamo WAMAC.\n",
      "Tu: nombre\n",
      "Confianza:0.5256704092025757 -> Intencion:CC_COMO_ESTAS\n",
      "WAMAC: No te entendi...\n",
      "Tu: nombre\n",
      "Confianza:0.5256704092025757 -> Intencion:CC_COMO_ESTAS\n",
      "WAMAC: No te entendi...\n",
      "Tu: nombre tu\n",
      "Confianza:0.9988136291503906 -> Intencion:CC_NAME_BOT\n",
      "WAMAC: No se si ya me pusieron mi nombre, pero escuche rumores de que me llamo WAMAC.\n",
      "Tu: hace frio\n",
      "Confianza:0.9997863173484802 -> Intencion:CC_EL_CLIMA\n",
      "WAMAC: Pues no se, no he aprendido sobre el clima todavia\n",
      "Tu: temperatura\n",
      "Confianza:0.9080889821052551 -> Intencion:CC_HOLA\n",
      "WAMAC: Hola!!\n",
      "Tu: temperatura de hoy\n",
      "Confianza:0.993463933467865 -> Intencion:CC_EL_CLIMA\n",
      "WAMAC: Pues no se, no he aprendido sobre el clima todavia\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHola! hablemos un rato... (type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to exit)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTu: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m tokenize(sentence)\n\u001b[0;32m     26\u001b[0m     X \u001b[38;5;241m=\u001b[39m bag_of_words(sentence, all_words)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m     )\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('chitchat_intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "bot_name = \"WAMAC\"\n",
    "print(\"Hola! hablemos un rato... (type 'quit' to exit)\")\n",
    "while True:\n",
    "    sentence = input(\"Tu: \")\n",
    "\n",
    "    sentence = tokenize(sentence)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    print(f\"Confianza:{prob.item()} -> Intencion:{tag}\")\n",
    "\n",
    "    if prob.item() > 0.7:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "        if tag == \"CC_ADIOS\":\n",
    "            break\n",
    "    else:\n",
    "        print(f\"{bot_name}: No te entendi...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb06c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397da84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89958903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1194f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c9a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
